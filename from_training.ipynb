{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61cd606",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import importlib\n",
    "import time\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "\n",
    "from absl import logging\n",
    "import chex\n",
    "from ferminet import checkpoint\n",
    "from ferminet import constants\n",
    "from ferminet import curvature_tags_and_blocks\n",
    "from ferminet import envelopes\n",
    "from ferminet import hamiltonian\n",
    "from ferminet import loss as qmc_loss_functions\n",
    "from ferminet import mcmc\n",
    "from ferminet import networks\n",
    "from ferminet import pretrain\n",
    "from ferminet.utils import multi_host\n",
    "from ferminet.utils import statistics\n",
    "from ferminet.utils import system\n",
    "from ferminet.utils import writers\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kfac_jax\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import optax\n",
    "from typing_extensions import Protocol\n",
    "from ferminet.configs import atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89db7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_electrons(\n",
    "        key,\n",
    "        molecule: Sequence[system.Atom],\n",
    "        electrons: Sequence[int],\n",
    "        batch_size: int,\n",
    "        init_width: float,\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Initializes electron positions around each atom.\n",
    "\n",
    "  Args:\n",
    "    key: JAX RNG state.\n",
    "    molecule: system.Atom objects making up the molecule.\n",
    "    electrons: tuple of number of alpha and beta electrons.\n",
    "    batch_size: total number of MCMC configurations to generate across all\n",
    "      devices.\n",
    "    init_width: width of (atom-centred) Gaussian used to generate initial\n",
    "      electron configurations.\n",
    "\n",
    "  Returns:\n",
    "    array of (batch_size, (nalpha+nbeta)*ndim) of initial (random) electron\n",
    "    positions in the initial MCMC configurations and ndim is the dimensionality\n",
    "    of the space (i.e. typically 3).\n",
    "  \"\"\"\n",
    "    if sum(atom.charge for atom in molecule) != sum(electrons):\n",
    "        if len(molecule) == 1:\n",
    "            atomic_spin_configs = [electrons]\n",
    "        else:\n",
    "            raise NotImplementedError('No initialization policy yet '\n",
    "                                      'exists for charged molecules.')\n",
    "    else:\n",
    "        atomic_spin_configs = [\n",
    "            (atom.element.nalpha, atom.element.nbeta) for atom in molecule\n",
    "        ]\n",
    "        assert sum(sum(x) for x in atomic_spin_configs) == sum(electrons)\n",
    "        while tuple(sum(x) for x in zip(*atomic_spin_configs)) != electrons:\n",
    "            i = np.random.randint(len(atomic_spin_configs))\n",
    "            nalpha, nbeta = atomic_spin_configs[i]\n",
    "            atomic_spin_configs[i] = nbeta, nalpha\n",
    "\n",
    "    # Assign each electron to an atom initially.\n",
    "    electron_positions = []\n",
    "    for i in range(2):\n",
    "        for j in range(len(molecule)):\n",
    "            atom_position = jnp.asarray(molecule[j].coords)\n",
    "            electron_positions.append(\n",
    "                jnp.tile(atom_position, atomic_spin_configs[j][i]))\n",
    "    electron_positions = jnp.concatenate(electron_positions)\n",
    "    # Create a batch of configurations with a Gaussian distribution about each\n",
    "    # atom.\n",
    "    key, subkey = jax.random.split(key)\n",
    "    return (\n",
    "            electron_positions +\n",
    "            init_width *\n",
    "            jax.random.normal(subkey, shape=(batch_size, electron_positions.size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3072cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizerState = Union[optax.OptState]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7bc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "OptUpdateResults = Tuple[networks.ParamTree, networks.ParamTree,  # param_psi, param_phi\n",
    "                         Optional[OptimizerState], Optional[OptimizerState],  # OptState\n",
    "                         jnp.ndarray,  # loss\n",
    "                         Optional[qmc_loss_functions.AuxiliaryLossData]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e50976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptUpdate(Protocol):\n",
    "\n",
    "    def __call__(self,\n",
    "                 params_psi: networks.ParamTree, data_psi: jnp.ndarray,\n",
    "                 params_phi: networks.ParamTree, data_phi: jnp.ndarray,\n",
    "                 params_previous: networks.ParamTree,\n",
    "                 opt_state_psi: optax.OptState, opt_state_phi: optax.OptState,\n",
    "                 key: chex.PRNGKey) -> OptUpdateResults:\n",
    "        \"\"\"Evaluates the loss and gradients and updates the parameters accordingly.\n",
    "\n",
    "    Args:\n",
    "      params_psi: network parameters.\n",
    "      params_phi: network parameters.\n",
    "      params_previous: network parameters.\n",
    "      data_psi: electron positions.\n",
    "      data_phi: electron positions.\n",
    "      opt_state: optimizer internal state.\n",
    "      key: RNG state.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (params_psi, params_phi, opt_state, loss, aux_data), where params and opt_state\n",
    "      are the updated parameters and optimizer state, loss is the evaluated loss\n",
    "      and aux_data auxiliary data (see AuxiliaryLossData docstring).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c02b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "StepResults = Tuple[jnp.ndarray, jnp.ndarray,  # data_psi 和 data_phi\n",
    "                    networks.ParamTree, networks.ParamTree,  # params_psi 和 params_phi\n",
    "                    Optional[optax.OptState],  Optional[optax.OptState],  # OptState\n",
    "                    jnp.ndarray, qmc_loss_functions.AuxiliaryLossData,  # loss, aux_data\n",
    "                    jnp.ndarray, jnp.ndarray]  # pmove_psi, pmove_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fb0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(Protocol):\n",
    "\n",
    "    def __call__(self,\n",
    "                 data_psi: jnp.ndarray,\n",
    "                 data_phi: jnp.ndarray,\n",
    "                 params_psi: networks.ParamTree,\n",
    "                 params_phi: networks.ParamTree,\n",
    "                 params_previous: networks.ParamTree,\n",
    "                 state_psi: OptimizerState,\n",
    "                 state_phi: OptimizerState,\n",
    "                 key: chex.PRNGKey,\n",
    "                 mcmc_width: jnp.ndarray) -> StepResults:\n",
    "        \"\"\"Performs one set of MCMC moves and an optimization step.\n",
    "\n",
    "    Args:\n",
    "      data: batch of MCMC configurations.\n",
    "      params: network parameters.\n",
    "      state: optimizer internal state.\n",
    "      key: JAX RNG state.\n",
    "      mcmc_width: width of MCMC move proposal. See mcmc.make_mcmc_step.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (data, params, state, loss, aux_data, pmove).\n",
    "        data: Updated MCMC configurations drawn from the network given the\n",
    "          *input* network parameters.\n",
    "        params: updated network parameters after the gradient update.\n",
    "        state: updated optimization state.\n",
    "        loss: energy of system based on input network parameters averaged over\n",
    "          the entire set of MCMC configurations.\n",
    "        aux_data: AuxiliaryLossData object also returned from evaluating the\n",
    "          loss of the system.\n",
    "        pmove: probability that a proposed MCMC move was accepted.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ec9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_update(params_psi: networks.ParamTree, data_psi: jnp.ndarray,\n",
    "                params_phi: networks.ParamTree, data_phi: jnp.ndarray,\n",
    "                params_previous: networks.ParamTree,\n",
    "                opt_state_psi: Optional[optax.OptState],\n",
    "                opt_state_phi: Optional[optax.OptState],\n",
    "                key: chex.PRNGKey) -> OptUpdateResults:\n",
    "    \"\"\"Performs an identity operation with an OptUpdate interface.\"\"\"\n",
    "    del data_psi, data_phi, key\n",
    "    return params_psi, params_phi, opt_state_psi, opt_state_phi, jnp.zeros(1), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aef000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_opt_update_step(evaluate_loss: qmc_loss_functions.LossFn,\n",
    "                         optimizer_psi: optax.GradientTransformation,\n",
    "                         optimizer_phi: optax.GradientTransformation,\n",
    "                         iteration_psi: int, iteration_phi: int) -> OptUpdate:\n",
    "    \"\"\"Returns an OptUpdate function for performing a parameter update.\"\"\"\n",
    "\n",
    "    # Differentiate wrt parameters (argument 0)\n",
    "    # loss_and_grad_psi = jax.value_and_grad(evaluate_loss, argnums=0, has_aux=True)\n",
    "    # loss_and_grad_phi = jax.value_and_grad(evaluate_loss, argnums=1, has_aux=True)\n",
    "\n",
    "    def opt_update(params_psi: networks.ParamTree, data_psi: jnp.ndarray,\n",
    "                   params_phi: networks.ParamTree, data_phi: jnp.ndarray,\n",
    "                   params_previous: networks.ParamTree,\n",
    "                   opt_state_psi: Optional[optax.OptState],\n",
    "                   opt_state_phi: Optional[optax.OptState],\n",
    "                   key: chex.PRNGKey) -> OptUpdateResults:\n",
    "        \"\"\"Evaluates the loss and gradients and updates the parameters using optax.\"\"\"\n",
    "        # 对loss进行closure操作\n",
    "        # 先psi下降一步\n",
    "        evaluate_loss_psi = lambda params, keys, data: \\\n",
    "            evaluate_loss(params, params_phi, params_previous, keys, data, data_phi)\n",
    "        loss_and_grad_psi = jax.value_and_grad(evaluate_loss_psi, argnums=0, has_aux=True)\n",
    "        \n",
    "        for k in range(iteration_psi):\n",
    "            (loss, aux_data), grad_psi = loss_and_grad_psi(params_psi, key, data_psi)\n",
    "            grad_psi = constants.pmean(grad_psi)\n",
    "            updates_psi, opt_state_psi = optimizer_psi.update(grad_psi, opt_state_psi, params_psi)\n",
    "            params_psi = optax.apply_updates(params_psi, updates_psi)\n",
    "\n",
    "        # 再对phi上升一步 这里有个负号\n",
    "        evaluate_loss_phi = lambda params, keys, data: \\\n",
    "            evaluate_loss(params_psi, params, params_previous, keys, data_psi, data)\n",
    "        loss_and_grad_phi = jax.value_and_grad(evaluate_loss_phi, argnums=0, has_aux=True)\n",
    "        \n",
    "        for k in range(iteration_phi):\n",
    "            (loss, aux_data), grad_phi = loss_and_grad_phi(params_phi, key, data_phi)\n",
    "            grad_phi = constants.pmean(grad_phi)\n",
    "            updates_phi, opt_state_phi = optimizer_phi.update(grad_phi, opt_state_phi, params_phi)\n",
    "            params_phi = optax.apply_updates(params_phi, updates_phi)\n",
    "            \n",
    "        #原先使用同一个优化器，所以要对updates乘以-1.updates为dict{list,list,dict,list,list}.\n",
    "        #现在改成两个优化器，在optax里面scale调整正负1就可以了\n",
    "        #for key in updates_phi:\n",
    "            #if type(key) == dict:\n",
    "                #for subkey in updates_phi[key]:\n",
    "                    #updates_phi[key][subkey] = -1 * updates_phi[key][subkey]\n",
    "            #if type(key) == list:\n",
    "                #updates_phi[key] = -1 * updates_phi[key]\n",
    "                \n",
    "        return params_psi, params_phi, opt_state_psi, opt_state_phi, loss, aux_data\n",
    "\n",
    "    return opt_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14cff518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss_step(evaluate_loss: qmc_loss_functions.LossFn) -> OptUpdate:\n",
    "    \"\"\"Returns an OptUpdate function for evaluating the loss.\"\"\"\n",
    "\n",
    "    def loss_eval(params_psi: networks.ParamTree, data_psi: jnp.ndarray,\n",
    "                  params_phi: networks.ParamTree, data_phi: jnp.ndarray,\n",
    "                  params_previous: networks.ParamTree,\n",
    "                  opt_state_psi: Optional[optax.OptState],\n",
    "                  opt_state_phi: Optional[optax.OptState],\n",
    "                  key: chex.PRNGKey) -> OptUpdateResults:\n",
    "        \"\"\"Evaluates just the loss and gradients with an OptUpdate interface.\"\"\"\n",
    "        loss, aux_data = evaluate_loss(params_psi, params_phi, params_previous, key, data_psi, data_phi)\n",
    "\n",
    "        return params_psi, params_phi, opt_state_psi, opt_state_phi, loss, aux_data\n",
    "\n",
    "    return loss_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40bce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_step(\n",
    "        mcmc_step,\n",
    "        optimizer_step: OptUpdate,\n",
    ") -> Step:\n",
    "    \"\"\"Factory to create traning step for non-KFAC optimizers.\n",
    "\n",
    "  Args:\n",
    "    mcmc_step: Callable which performs the set of MCMC steps. See make_mcmc_step\n",
    "      for creating the callable.\n",
    "    optimizer_step: OptUpdate callable which evaluates the forward and backward\n",
    "      passes and updates the parameters and optimizer state, as required.\n",
    "\n",
    "  Returns:\n",
    "    step, a callable which performs a set of MCMC steps and then an optimization\n",
    "    update. See the Step protocol for details.\n",
    "  \"\"\"\n",
    "\n",
    "    # 这个修饰是干啥的\n",
    "    #@functools.partial(constants.pmap, donate_argnums=(0, 1, 2, 3, 4, 5, 6))\n",
    "    @constants.pmap\n",
    "    def step(data_psi: jnp.ndarray, data_phi: jnp.ndarray,\n",
    "             params_psi: networks.ParamTree, params_phi: networks.ParamTree, params_previous: networks.ParamTree,\n",
    "             state_psi: Optional[optax.OptState],\n",
    "             state_phi: Optional[optax.OptState],\n",
    "             key: chex.PRNGKey, mcmc_width: jnp.ndarray) -> StepResults:\n",
    "        \"\"\"A full update iteration (except for KFAC): MCMC steps + optimization.\"\"\"\n",
    "        # MCMC loop for psi\n",
    "        mcmc_key, loss_key = jax.random.split(key, num=2)\n",
    "        data_psi, pmove_psi = mcmc_step(params_psi, data_psi, mcmc_key, mcmc_width)\n",
    "        # MCMC loop for phi\n",
    "        mcmc_key, loss_key = jax.random.split(key, num=2)\n",
    "        data_phi, pmove_phi = mcmc_step(params_phi, data_phi, mcmc_key, mcmc_width)\n",
    "\n",
    "        # Optimization step for psi&phi 需要提前在optimizer里调整好内循环的次数\n",
    "        new_params_psi, new_params_phi, state_psi, state_phi, loss, aux_data = \\\n",
    "            optimizer_step(params_psi, data_psi, params_phi, data_phi, params_previous, state_psi, state_phi, loss_key)\n",
    "\n",
    "        result: Tuple\n",
    "        result = [data_psi, data_phi,\n",
    "                  new_params_psi, new_params_phi,\n",
    "                  state_psi, state_phi,\n",
    "                  loss, aux_data, pmove_psi, pmove_phi]\n",
    "        return result\n",
    "\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74e375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_manager=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8b06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = atom.get_config()\n",
    "cfg.system.atom = 'H'\n",
    "cfg.system.spin_polarisation = None\n",
    "cfg = atom._adjust_nuclear_charge(cfg)\n",
    "cfg.batch_size = 128\n",
    "cfg.pretrain.iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122b70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = jax.local_device_count()\n",
    "num_hosts = jax.device_count() // num_devices\n",
    "logging.info('Starting QMC with %i XLA devices per host '\n",
    "                 'across %i hosts.', num_devices, num_hosts)\n",
    "if cfg.batch_size % (num_devices * num_hosts) != 0:\n",
    "    raise ValueError('Batch size must be divisible by number of devices, '\n",
    "                         f'got batch size {cfg.batch_size} for '\n",
    "                         f'{num_devices * num_hosts} devices.')\n",
    "host_batch_size = cfg.batch_size // num_hosts  # batch size per host\n",
    "device_batch_size = host_batch_size // num_devices  # batch size per device\n",
    "data_shape = (num_devices, device_batch_size)\n",
    "\n",
    " # Check if mol is a pyscf molecule and convert to internal representation\n",
    "if cfg.system.pyscf_mol:\n",
    "    cfg.update(\n",
    "        system.pyscf_mol_to_internal_representation(cfg.system.pyscf_mol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17c81fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = jnp.stack([jnp.array(atom.coords) for atom in cfg.system.molecule])\n",
    "charges = jnp.array([atom.charge for atom in cfg.system.molecule])\n",
    "nspins = cfg.system.electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a5ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.debug.deterministic:\n",
    "    seed = 23\n",
    "else:\n",
    "    seed = 1e6 * time.time()\n",
    "    seed = int(multi_host.broadcast_to_hosts(seed))\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50c7128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.pretrain.method == 'direct_init' or (\n",
    "        cfg.pretrain.method == 'hf' and cfg.pretrain.iterations > 0):\n",
    "    hartree_fock = pretrain.get_hf(\n",
    "        pyscf_mol=cfg.system.get('pyscf_mol'),\n",
    "        molecule=cfg.system.molecule,\n",
    "        nspins=nspins,\n",
    "        restricted=False,\n",
    "        basis=cfg.pretrain.basis)\n",
    "    # broadcast the result of PySCF from host 0 to all other hosts\n",
    "    hartree_fock.mean_field.mo_coeff = tuple([\n",
    "        multi_host.broadcast_to_hosts(x)\n",
    "        for x in hartree_fock.mean_field.mo_coeff\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae6a8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_solution = hartree_fock if cfg.pretrain.method == 'direct_init' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ac98ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.network.make_feature_layer_fn:\n",
    "    feature_layer_module, feature_layer_fn = (\n",
    "        cfg.network.make_feature_layer_fn.rsplit('.', maxsplit=1))\n",
    "    feature_layer_module = importlib.import_module(feature_layer_module)\n",
    "    make_feature_layer = getattr(feature_layer_module, feature_layer_fn)\n",
    "    feature_layer = make_feature_layer(\n",
    "        charges,\n",
    "        cfg.system.electrons,\n",
    "        cfg.system.ndim,\n",
    "        **cfg.network.make_feature_layer_kwargs)  # type: networks.FeatureLayer\n",
    "else:\n",
    "    feature_layer = networks.make_ferminet_features(\n",
    "        charges,\n",
    "        cfg.system.electrons,\n",
    "        cfg.system.ndim,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec586faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.network.make_envelope_fn:\n",
    "    envelope_module, envelope_fn = (\n",
    "        cfg.network.make_envelope_fn.rsplit('.', maxsplit=1))\n",
    "    envelope_module = importlib.import_module(envelope_module)\n",
    "    make_envelope = getattr(envelope_module, envelope_fn)\n",
    "    envelope = make_envelope(**cfg.network.make_envelope_kwargs)  # type: envelopes.Envelope\n",
    "else:\n",
    "    envelope = envelopes.make_isotropic_envelope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35459471",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_init, signed_network, network_options = networks.make_fermi_net(\n",
    "    atoms,\n",
    "    nspins,\n",
    "    charges,\n",
    "    envelope=envelope,\n",
    "    feature_layer=feature_layer,\n",
    "    bias_orbitals=cfg.network.bias_orbitals,\n",
    "    use_last_layer=cfg.network.use_last_layer,\n",
    "    hf_solution=hf_solution,\n",
    "    full_det=cfg.network.full_det,\n",
    "    ndim=cfg.system.ndim,\n",
    "    **cfg.network.detnet)\n",
    "key, subkey = jax.random.split(key)\n",
    "params_psi = network_init(subkey)\n",
    "params_psi = kfac_jax.utils.replicate_all_local_devices(params_psi)\n",
    "key, subkey = jax.random.split(key)\n",
    "params_phi = network_init(subkey)\n",
    "params_phi = kfac_jax.utils.replicate_all_local_devices(params_phi)\n",
    "# Often just need log|psi(x)|.\n",
    "network = lambda *args, **kwargs: signed_network(*args, **kwargs)[1]  # type: networks.LogFermiNetLike\n",
    "batch_network = jax.vmap(\n",
    "    network, in_axes=(None, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de15bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里改成对psi和phi都设置存储和读取路径\n",
    "ckpt_save_path = checkpoint.create_save_path(cfg.log.save_path)\n",
    "ckpt_restore_path = checkpoint.get_restore_path(cfg.log.restore_path)\n",
    "ckpt_restore_filename = (\n",
    "        checkpoint.find_last_checkpoint(ckpt_save_path) or\n",
    "        checkpoint.find_last_checkpoint(ckpt_restore_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30d36ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('No checkpoint found. Training new model.')\n",
    "key, subkey = jax.random.split(key)\n",
    "# make sure data on each host is initialized differently\n",
    "subkey = jax.random.fold_in(subkey, jax.process_index())\n",
    "data_psi = init_electrons(\n",
    "    subkey,\n",
    "    cfg.system.molecule,\n",
    "    cfg.system.electrons,\n",
    "    batch_size=host_batch_size,\n",
    "    init_width=cfg.mcmc.init_width)\n",
    "data_psi = jnp.reshape(data_psi, data_shape + data_psi.shape[1:])\n",
    "data_psi = kfac_jax.utils.broadcast_all_local_devices(data_psi)\n",
    "key, subkey = jax.random.split(key)\n",
    "# make sure data on each host is initialized differently\n",
    "subkey = jax.random.fold_in(subkey, jax.process_index())\n",
    "data_phi = init_electrons(\n",
    "    subkey,\n",
    "    cfg.system.molecule,\n",
    "    cfg.system.electrons,\n",
    "    batch_size=host_batch_size,\n",
    "    init_width=cfg.mcmc.init_width)\n",
    "data_phi = jnp.reshape(data_phi, data_shape + data_phi.shape[1:])\n",
    "data_phi = kfac_jax.utils.broadcast_all_local_devices(data_phi)\n",
    "t_init = 0\n",
    "opt_state_ckpt = None\n",
    "mcmc_width_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "460a4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schema = ['step', 'energy', 'ewmean', 'ewvar', 'pmove_psi', 'pmove_phi']\n",
    "\n",
    "# Initialisation done. We now want to have different PRNG streams on each\n",
    "# device. Shard the key over devices\n",
    "sharded_key = kfac_jax.utils.make_different_rng_key_on_all_devices(key)\n",
    "\n",
    "# Pretraining to match Hartree-Fock\n",
    "\n",
    "if (t_init == 0 and cfg.pretrain.method == 'hf' and\n",
    "        cfg.pretrain.iterations > 0):\n",
    "    orbitals = functools.partial(\n",
    "        networks.fermi_net_orbitals,\n",
    "        atoms=atoms,\n",
    "        nspins=cfg.system.electrons,\n",
    "        options=network_options,\n",
    "    )\n",
    "    batch_orbitals = jax.vmap(\n",
    "        lambda params, data: orbitals(params, data)[0],\n",
    "        in_axes=(None, 0),\n",
    "        out_axes=0)\n",
    "    sharded_key, subkeys = kfac_jax.utils.p_split(sharded_key)\n",
    "    params_psi, data_psi = pretrain.pretrain_hartree_fock(\n",
    "        params=params_psi,\n",
    "        data=data_psi,\n",
    "        batch_network=batch_network,\n",
    "        batch_orbitals=batch_orbitals,\n",
    "        network_options=network_options,\n",
    "        sharded_key=subkeys,\n",
    "        atoms=atoms,\n",
    "        electrons=cfg.system.electrons,\n",
    "        scf_approx=hartree_fock,\n",
    "        iterations=cfg.pretrain.iterations)\n",
    "    sharded_key, subkeys = kfac_jax.utils.p_split(sharded_key)\n",
    "    params_phi, data_phi = pretrain.pretrain_hartree_fock(\n",
    "        params=params_phi,\n",
    "        data=data_phi,\n",
    "        batch_network=batch_network,\n",
    "        batch_orbitals=batch_orbitals,\n",
    "        network_options=network_options,\n",
    "        sharded_key=subkeys,\n",
    "        atoms=atoms,\n",
    "        electrons=cfg.system.electrons,\n",
    "        scf_approx=hartree_fock,\n",
    "        iterations=cfg.pretrain.iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b5719f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms_to_mcmc = atoms if cfg.mcmc.scale_by_nuclear_distance else None\n",
    "mcmc_step = mcmc.make_mcmc_step(\n",
    "    batch_network,\n",
    "    device_batch_size,\n",
    "    steps=cfg.mcmc.steps,\n",
    "    atoms=atoms_to_mcmc,\n",
    "    one_electron_moves=cfg.mcmc.one_electron,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "768b9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_energy = hamiltonian.local_energy(\n",
    "    f=signed_network,\n",
    "    atoms=atoms,\n",
    "    charges=charges,\n",
    "    nspins=nspins,\n",
    "    use_scan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "805127ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_loss = qmc_loss_functions.make_loss(\n",
    "    network,\n",
    "    local_energy,\n",
    "    clip_local_energy=cfg.optim.clip_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "287a61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(t_: jnp.ndarray) -> jnp.ndarray:\n",
    "    return cfg.optim.lr.rate * jnp.power(\n",
    "        (1.0 / (1.0 + (t_ / cfg.optim.lr.delay))), cfg.optim.lr.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c41e8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对两个优化器可以定义不同的schedule\n",
    "optimizer_psi = optax.chain(\n",
    "    optax.scale_by_adam(**cfg.optim.adam),\n",
    "    optax.scale_by_schedule(learning_rate_schedule),\n",
    "    optax.scale(-1.))\n",
    "optimizer_phi = optax.chain(\n",
    "    optax.scale_by_adam(**cfg.optim.adam),\n",
    "    optax.scale_by_schedule(learning_rate_schedule),\n",
    "    optax.scale(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a03b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state_psi = jax.pmap(optimizer_psi.init)(params_psi)\n",
    "opt_state_psi = opt_state_ckpt or opt_state_psi  # avoid overwriting ckpted state\n",
    "opt_state_phi = jax.pmap(optimizer_phi.init)(params_phi)\n",
    "opt_state_phi = opt_state_ckpt or opt_state_phi  # avoid overwriting ckpted state\n",
    "\n",
    "# 定义内循环步数\n",
    "k_psi = cfg.optim.iterations_psi\n",
    "k_phi = cfg.optim.iterations_phi\n",
    "\n",
    "# 这段注意一下\n",
    "step = make_training_step(\n",
    "    mcmc_step=mcmc_step,\n",
    "    optimizer_step=make_opt_update_step(evaluate_loss, optimizer_psi, optimizer_phi, k_psi, k_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bda0413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mcmc_width_ckpt is not None:\n",
    "    mcmc_width = kfac_jax.utils.replicate_all_local_devices(mcmc_width_ckpt[0])\n",
    "else:\n",
    "    mcmc_width = kfac_jax.utils.replicate_all_local_devices(\n",
    "        jnp.asarray(cfg.mcmc.move_width))\n",
    "pmoves = np.zeros(cfg.mcmc.adapt_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f626e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_init == 0:\n",
    "    logging.info('Burning in MCMC chain for %d steps', cfg.mcmc.burn_in)\n",
    "\n",
    "    burn_in_step = make_training_step(\n",
    "        mcmc_step=mcmc_step, optimizer_step=null_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ad12d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.optim.iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4858433",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_previous = params_psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28a1db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.mcmc.burn_in = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1fbb167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in range(cfg.mcmc.burn_in):\n",
    "    sharded_key, subkeys = kfac_jax.utils.p_split(sharded_key)\n",
    "    data_psi, data_phi, params_psi, params_phi, *_ = burn_in_step(\n",
    "        data_psi,\n",
    "        data_phi,\n",
    "        params_psi,\n",
    "        params_phi,\n",
    "        params_previous,\n",
    "        state_psi=None,\n",
    "        state_phi=None,\n",
    "        key=subkeys,\n",
    "        mcmc_width=mcmc_width)\n",
    "logging.info('Completed burn-in MCMC steps')\n",
    "\n",
    "sharded_key, subkeys = kfac_jax.utils.p_split(sharded_key)\n",
    "ptotal_energy = constants.pmap(evaluate_loss)\n",
    "initial_energy, _ = ptotal_energy(params_psi, params_psi, params_previous, \\\n",
    "                                  subkeys, data_psi, data_phi)\n",
    "logging.info('Initial energy: %03.4f E_h', initial_energy[0])\n",
    "\n",
    "time_of_last_ckpt = time.time()\n",
    "weighted_stats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2356f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.04710469], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b42ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.optim.optimizer == 'none' and opt_state_ckpt is not None:\n",
    " # If opt_state_ckpt is None, then we're restarting from a previous inference\n",
    " # run (most likely due to preemption) and so should continue from the last\n",
    " # iteration in the checkpoint. Otherwise, starting an inference run from a\n",
    " # training run.\n",
    "    logging.info('No optimizer provided. Assuming inference run.')\n",
    "    logging.info('Setting initial iteration to 0.')\n",
    "    t_init = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f52342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb9b6aad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.72421] [0.98046875] [1.]\n",
      "run time is 1686732724.3779762\n",
      "[-0.11365134] [0.9828125] [0.99921876]\n",
      "run time is 1686732724.56288\n",
      "[74063.055] [0.97734374] [0.9976563]\n",
      "run time is 1686732724.790601\n",
      "[-44.3219] [0.94687504] [0.94453126]\n",
      "run time is 1686732724.96032\n",
      "[-18.900875] [0.93828124] [0.9632813]\n",
      "run time is 1686732725.146911\n",
      "[358.43237] [0.9640625] [0.9765625]\n",
      "run time is 1686732725.3169038\n",
      "[1871.159] [0.9484375] [0.975]\n",
      "run time is 1686732725.513228\n",
      "[256.26468] [0.9484375] [0.97265625]\n",
      "run time is 1686732725.693111\n",
      "[-93.36137] [0.94687504] [0.97578126]\n",
      "run time is 1686732725.932626\n",
      "[54.462776] [0.97812504] [0.9742188]\n",
      "run time is 1686732726.190337\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for t in range(t_init, cfg.optim.iterations):\n",
    "    start = time.time()\n",
    "    sharded_key, subkeys = kfac_jax.utils.p_split(sharded_key)\n",
    "    data_psi, data_phi, params_psi, params_phi, \\\n",
    "    opt_state_psi, opt_state_phi, loss, unused_aux_data, \\\n",
    "    pmove_psi, pmove_phi = step(\n",
    "        data_psi, data_phi,\n",
    "        params_psi, params_phi, params_previous,\n",
    "        opt_state_psi, opt_state_phi,\n",
    "        subkeys,\n",
    "        mcmc_width)\n",
    "    print(loss, pmove_psi, pmove_phi)\n",
    "    end = time.time()\n",
    "    print('run time is ' + str(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d4ead53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.996875], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmove_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ba334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
